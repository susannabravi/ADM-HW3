{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. We strongly suggest you work on different modules when implementing the required functions. For example, you may have a ```crawler.py``` module, a ```parser.py``` module, and a ```engine.py``` module: this is a good practice that improves readability in reporting and efficiency in deploying the code. Be careful; you are likely dealing with exceptions and other possible issues! \n",
    "\n",
    "### 1.1. Get the list of master's degree courses\n",
    "\n",
    "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the [MSc Degrees](https://www.findamasters.com/masters-degrees/msc-degrees/). Next, we want you to **collect the URL** associated with each site in the list from the previously collected list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the master's URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"urls.txt\",\"w\") # First I create a txt file where I can write the URLs  #  w means writing mode\n",
    "for i in range(1, 401): #from first page to page 400\n",
    "    url = f\"https://www.findamasters.com/masters-degrees/msc-degrees/?PG={i}\" #pages can be scrolled by changing the number after PG\n",
    "    result = requests.get(url) # as we have done in class\n",
    "    soup = BeautifulSoup(result.text, 'html.parser') # to get the html of each page\n",
    "\n",
    "    for link in soup.find_all(class_ = re.compile('courseLink')): #as in class to get each tag of the page which belongs to class courseLink\n",
    "        c = (link.get(\"href\"))  # url of each page in the i-th page\n",
    "        f.write(\"https://www.findamasters.com/\"+c) #writing the rows\n",
    "        f.write(\"\\n\")\n",
    "f.close()\n",
    "print('The \"urls.txt\" file is generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl master's degree pages\n",
    "\n",
    "We wrote a function named 'download_url'.\n",
    "Since the FindMaster website blocks us for 70 seconds for every (20 to 22) requests we send, we use 'time.sleep(70)' to wait and then resend the http get request. \n",
    "we also omit to download the http files that their directory are already existed.\n",
    "\n",
    "for sending http get requests asynchronously, we can use async and await methods and take the advantage of using \"aiohttp\" library. the other way is to use ThreadPoolExecutor function executer. \n",
    "It means that we store the executer command in a variable named 'future_to_url' that we are able to call in the future.\n",
    "The ThreadPoolExecutor is a built-in Python module that provides managing a pool of worker threads. It allows us to submit tasks to the pool, which are then executed by one of the worker threads in the pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded page 1189: https://www.findamasters.com//masters-degrees/course/applied-research-msc/?i228d1972c64128\n",
      "Downloaded page 1190: https://www.findamasters.com//masters-degrees/course/applied-sciences-and-engineering-applied-computer-science/?i385d6811c25883\n",
      "Downloaded page 1191: https://www.findamasters.com//masters-degrees/course/applied-sciences-and-engineering-computer-science/?i385d6809c25959\n",
      "Downloaded page 1192: https://www.findamasters.com//masters-degrees/course/applied-social-and-political-psychology-msc/?i131d8689c41516\n",
      "Downloaded page 1193: https://www.findamasters.com//masters-degrees/course/applied-social-data-science-msc/?i150d2843c50489\n",
      "Downloaded page 1194: https://www.findamasters.com//masters-degrees/course/applied-social-research-msc/?i250d4598c11311\n",
      "Downloaded page 1195: https://www.findamasters.com//masters-degrees/course/applied-social-research-msc/?i352d8078c21667\n",
      "Downloaded page 1196: https://www.findamasters.com//masters-degrees/course/applied-social-science-msc/?i360d5482c52405\n",
      "Downloaded page 1197: https://www.findamasters.com//masters-degrees/course/applied-social-sciences-globalisation-and-crime-msc/?i255d4447c65800\n",
      "Downloaded page 1198: https://www.findamasters.com//masters-degrees/course/applied-social-work-practice-msc-pgdip-pgcert/?i347d663c44032\n",
      "Downloaded page 1199: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-biomechanics/?i225d101c59742\n",
      "Downloaded page 1200: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-medicine-msc/?i338d6585c43963\n",
      "Downloaded page 1201: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-nutrition-pgdip-msc/?i188d6126c5296\n",
      "Downloaded page 1202: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-nutrition-msc/?i293d7150c55423\n",
      "Downloaded page 1203: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-physiology-msc-pgdip/?i13d8008c39492\n",
      "Downloaded page 1204: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-physiology-msc-pgdip-pgcert/?i225d101c24212\n",
      "Downloaded page 1205: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-science-msc-ma/?i337d1774c64739\n",
      "Downloaded page 1206: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-science-msc/?i295d7133c41201\n",
      "Downloaded page 1207: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-science-msc/?i219d2021c4897\n",
      "Downloaded page 1208: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-science-msc/?i228d1972c56524\n",
      "Downloaded page 1209: https://www.findamasters.com//masters-degrees/course/applied-sport-and-exercise-sciences-with-specialism-msc/?i298d4201c12552\n",
      "Downloaded page 1210: https://www.findamasters.com//masters-degrees/course/applied-sport-physiology-msc-pgcert-pgdip/?i286d1653c33082\n",
      "Downloaded page 1211: https://www.findamasters.com//masters-degrees/course/applied-sport-psychology-msc-pgdip-pgcert/?i225d101c27842\n",
      "Downloaded page 1212: https://www.findamasters.com//masters-degrees/course/applied-sport-science-msc-pgdip-pgcert/?i13d8008c39494\n",
      "Downloaded page 1213: https://www.findamasters.com//masters-degrees/course/applied-sport-science-msc/?i374d6037c41441\n",
      "Downloaded page 1214: https://www.findamasters.com//masters-degrees/course/applied-sports-coaching-msc/?i324d4570c58607\n",
      "Downloaded page 1215: https://www.findamasters.com//masters-degrees/course/applied-sports-nutrition-msc-pgdip-pgcert/?i225d101c29664\n",
      "Failed to download page 1216: https://www.findamasters.com//masters-degrees/course/applied-sports-performance-analysis-msc/?i374d6037c47566\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.findamasters.com//masters-degrees/course/applied-sports-performance-analysis-msc/?i374d6037c47566\n",
      "Retrying in 70 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Function to download and save HTML for a given URL\n",
    "def download_url(url, folder_path, page_number):\n",
    "    # Create a folder for each page if it doesn't exist\n",
    "    page_folder = os.path.join(folder_path, f\"page_{page_number}\")\n",
    "    if os.path.exists(page_folder):\n",
    "        # uncomment the below code to see which pages are skiped, cause they have already been downloaded.\n",
    "        # print(f\"Skipping Page: {page_number} - Folder already exists.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url) # Send a GET request to the URL\n",
    "        response.raise_for_status()  # Raise an exception for bad responses \n",
    "\n",
    "        # Create a folder for each page if it doesn't exist\n",
    "        os.makedirs(page_folder, exist_ok=True)\n",
    "\n",
    "        # Save the HTML content to a file\n",
    "        file_path = os.path.join(page_folder, f\"html_{page_number}.html\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"Downloaded page {page_number}: {url}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download page {page_number}: {url}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Retrying in 70 seconds...\")\n",
    "        time.sleep(70)  # Wait for 10 seconds before retrying\n",
    "        download_url(url, folder_path, page_number)  # Retry the download\n",
    "\n",
    "# Read all URLs one by one\n",
    "with open('urls.txt', 'r') as urls_file:\n",
    "    urls = urls_file.read().splitlines()\n",
    "\n",
    "output_folder = 'HTML_folders' # Store all HTML files into this directory.\n",
    "\n",
    "# We can use ThreadPoolExecutor for sending http requests asynchronously. \n",
    "# However, Since the FindMaster website blocks us for 70 seconds for every (20 to 22) requests we send, \n",
    "# the max_workers in below code assigned to number 1. So it sends requests synchronously.\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    # Enumerate through each URL and submit download tasks to the executor\n",
    "    future_to_url = {executor.submit(download_url, url, output_folder, page_number): url for page_number, url in enumerate(urls, start=1)}\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
